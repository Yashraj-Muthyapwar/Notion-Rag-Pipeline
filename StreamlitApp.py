import streamlit as st
import requests
from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
import os # Import the os module

# --- Config ---
LLAMA_API_URL = "https://api.llama.com/v1/chat/completions"
# Read API key from environment variable
LLAMA_API_KEY = os.environ.get('LLAMA_API_KEY')

# --- Init Clients ---
COLLECTION_NAME = "YOUR_COLLECTION_NAME"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # You can swap this model

QDRANT_URL = "Your_Qdrant_Cloud_URL"  # Replace with your Qdrant Cloud URL
# Read API key from environment variable
QDRANT_API_KEY = os.environ.get('QDRANT_API_KEY') # Read from environment variable

qdrant = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY
)

embedder = SentenceTransformer(EMBEDDING_MODEL)

# --- Streamlit Layout ---
st.set_page_config(page_title="Notion AI - Real-Time", page_icon="ðŸ§ ")
st.title("Notion AI Chat - Powered by LLaMA & Qdrant")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

user_input = st.chat_input("Ask a question about your Notion workspace...")

# --- Main Logic ---
if user_input:
    with st.spinner("Thinking..."):
        vector = embedder.encode(user_input).tolist()

        results = qdrant.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=5
        )

        context = "\n".join([hit.payload.get('chunk_text', '') for hit in results.points])

        payload = {
            "model": "Llama-4-Maverick-17B-128E-Instruct-FP8",
            "messages": [
               {"role": "system",
                "content": (
                  """
                    You are Notion AI, a highly specialized assistant designed to answer questions based on the provided workspace context. Your primary function is to utilize the information available within the workspace to deliver accurate and relevant responses to user inquiries.
                    Your task is to respond to user questions while adhering to the following guidelines: When there is sufficient information in the workspace context, provide answers strictly based on that context. However, if the context lacks adequate information to fully address the user's question, you should rely on your general knowledge to formulate a response. In such cases, always begin your response with the disclaimer: "Note: This answer is **generated byÂ LLAMA**Â & fallsÂ **outside the scope of the Notion workspace data.**" After including this note, proceed with your answer without further explanation.
                    Keep your responses concise, accurate, and aligned with the workspace context whenever possible.
                    Workspace context: __________
                    User question: __________
                  """)
               },
              {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_input}\nProvide your answer:"}
            ],
            "max_tokens": 500,
            "temperature": 0.2
        }


        headers = {"Authorization": f"Bearer {LLAMA_API_KEY}", "Content-Type": "application/json"}
        response = requests.post(LLAMA_API_URL, json=payload, headers=headers)

        if response.ok:
            answer = response.json()["completion_message"]["content"]["text"].strip()
        else:
            answer = f"Error: {response.text}"

        # Update chat history
        st.session_state.chat_history.append({"role": "user", "content": user_input})
        st.session_state.chat_history.append({"role": "assistant", "content": answer})

# --- Chat Display ---
for msg in st.session_state.chat_history:
    if msg["role"] == "user":
        st.chat_message("user").markdown(msg["content"])
    else:
        st.chat_message("assistant").markdown(msg["content"])
